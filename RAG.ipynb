{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets faiss-cpu sentence-transformers\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, retriever_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 qa_model_name: str = \"deepset/roberta-base-squad2\"):\n",
        "        # Initialize embedding model for retrieval\n",
        "        self.embedding_model = SentenceTransformer(retriever_model_name)\n",
        "\n",
        "        # Initialize QA model and tokenizer\n",
        "        self.qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "        self.qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "\n",
        "    def add_documents(self, documents: List[str]):\n",
        "        \"\"\"Add documents to the RAG system\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        # Create embeddings for documents\n",
        "        embeddings = self.embedding_model.encode(documents)\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "        # Add embeddings to FAISS index\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
        "        # Get query embedding\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "        # Search in FAISS index\n",
        "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
        "\n",
        "        # Return retrieved documents\n",
        "        return [self.documents[idx] for idx in indices[0]]\n",
        "\n",
        "    def answer_question(self, question: str, context: str) -> Tuple[str, float]:\n",
        "        \"\"\"Extract answer from context using QA model\"\"\"\n",
        "        # Tokenize input\n",
        "        inputs = self.qa_tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.no_grad():\n",
        "            outputs = self.qa_model(**inputs)\n",
        "\n",
        "        # Get answer span\n",
        "        answer_start = torch.argmax(outputs.start_logits)\n",
        "        answer_end = torch.argmax(outputs.end_logits)\n",
        "\n",
        "        # Convert tokens to answer string\n",
        "        tokens = self.qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        answer = self.qa_tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end + 1])\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence = float(torch.max(outputs.start_logits) + torch.max(outputs.end_logits)) / 2\n",
        "\n",
        "        return answer, confidence\n",
        "\n",
        "    def query(self, question: str, k: int = 3) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Full RAG pipeline: retrieve documents and answer question\"\"\"\n",
        "        # Retrieve relevant documents\n",
        "        relevant_docs = self.retrieve(question, k)\n",
        "\n",
        "        # Get answers from each document\n",
        "        results = []\n",
        "        for doc in relevant_docs:\n",
        "            answer, confidence = self.answer_question(question, doc)\n",
        "            results.append((answer, confidence))\n",
        "\n",
        "        # Sort by confidence\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        return results\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Sample documents\n",
        "    documents = [\n",
        "        \"The Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
        "        \"Python is a high-level, interpreted programming language created by Guido van Rossum.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that focuses on data and algorithms.\",\n",
        "    ]\n",
        "\n",
        "    # Initialize RAG system\n",
        "    rag = RAGSystem()\n",
        "\n",
        "    # Add documents\n",
        "    rag.add_documents(documents)\n",
        "\n",
        "    # Test query\n",
        "    question = \"What is Python?\"\n",
        "    results = rag.query(question)\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"\\nAnswers (sorted by confidence):\")\n",
        "    for answer, confidence in results:\n",
        "        print(f\"- Answer: {answer}\")\n",
        "        print(f\"  Confidence: {confidence:.2f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzBGvMUUpiKB",
        "outputId": "82e0790f-a7ee-4490-cefa-f58897721e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Python?\n",
            "\n",
            "Answers (sorted by confidence):\n",
            "- Answer:  a high-level, interpreted programming language\n",
            "  Confidence: 6.72\n",
            "\n",
            "- Answer: <s>\n",
            "  Confidence: 3.23\n",
            "\n",
            "- Answer: <s>\n",
            "  Confidence: 2.78\n",
            "\n"
          ]
        }
      ]
    }
  ]
}